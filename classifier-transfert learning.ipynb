{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms, models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1r/8rym71sj2cq1cz996pm7qd9h0000gn/T/ipykernel_93324/1935671458.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/bastianchuttarsing/Documents/projet_mondor/data_png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m train_transforms = transforms.Compose([\n\u001b[0m\u001b[1;32m      5\u001b[0m                                 \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomResizedCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                 \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomHorizontalFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "data_dir = \"/Users/bastianchuttarsing/Documents/projet_mondor/data_png\"\n",
    "train_transforms = transforms.Compose([\n",
    "                                transforms.RandomResizedCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "test_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "train_dataset = datasets.ImageFolder(data_dir + '/train',  \n",
    "                                    transform=train_transforms)                                       \n",
    "test_dataset = datasets.ImageFolder(data_dir + '/test', \n",
    "                                    transform=test_transforms)\n",
    "\n",
    "#Data Loading\n",
    "\n",
    "classes = ('cck', 'mixtes')\n",
    "\n",
    "m=len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation (test)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = \"/Users/bastianchuttarsing/Documents/projet_mondor/data_png/\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of classes: \n",
      " {'cck': 468, 'mixte': 595}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "target_list = torch.tensor(train_dataset.targets)\n",
    "target_list = target_list[torch.randperm(len(target_list))]\n",
    "\n",
    "train_dataset.class_to_idx\n",
    "\n",
    "idx2class = {v: k for k, v in train_dataset.class_to_idx.items()}\n",
    "idx2class\n",
    "\n",
    "def get_class_distribution(dataset_obj):\n",
    "    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}\n",
    "    \n",
    "    for element in dataset_obj:\n",
    "        y_lbl = element[1]\n",
    "        y_lbl = idx2class[y_lbl]\n",
    "        count_dict[y_lbl] += 1\n",
    "            \n",
    "    return count_dict\n",
    "print(\"Distribution of classes: \\n\", get_class_distribution(train_dataset))\n",
    "\n",
    "\n",
    "class_count = [i for i in get_class_distribution(train_dataset).values()]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
    "class_weights\n",
    "\n",
    "class_weights_all = class_weights[target_list]\n",
    "#class_weights_all\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=class_weights_all,\n",
    "    num_samples=len(class_weights_all),\n",
    "    replacement=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=15,  sampler=weighted_sampler)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "#num_ftrs = model_ft.classifier.in_features\n",
    "#model_ft_classifier = nn.Linear(num_ftrs, len(class_names))\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "num_epochs=3\n",
    "batch_size=16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,device,dataloader,loss_fn,optimizer):\n",
    "    train_loss,train_correct=0.0,0\n",
    "    model.train()\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        scores, predictions = torch.max(output.data, 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    return train_loss,train_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model,device,dataloader,loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    for images, labels in dataloader:\n",
    "\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        output = model(images)\n",
    "        loss=loss_fn(output,labels)\n",
    "        valid_loss+=loss.item()*images.size(0)\n",
    "        scores, predictions = torch.max(output.data,1)\n",
    "        val_correct+=(predictions == labels).sum().item()\n",
    "\n",
    "    return valid_loss,val_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1r/8rym71sj2cq1cz996pm7qd9h0000gn/T/ipykernel_91414/1198875798.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1r/8rym71sj2cq1cz996pm7qd9h0000gn/T/ipykernel_91414/3612049671.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_correct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_correct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1r/8rym71sj2cq1cz996pm7qd9h0000gn/T/ipykernel_91414/2718200997.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, device, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PATH = f'/content/drive/MyDrive/tp\\ DL/{num_epochs}_{model.__class__.__name__}.pth'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "    \n",
    "\n",
    "history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_correct=train_epoch(model,device,train_loader,criterion,optimizer)\n",
    "    test_loss, test_correct=valid_epoch(model,device,test_loader,criterion)\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "    test_loss = test_loss / len(test_loader.sampler)\n",
    "    test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "    print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                          num_epochs,\n",
    "                                                                                                          train_loss,\n",
    "                                                                                                          test_loss,\n",
    "                                                                                                          train_acc,\n",
    "                                                                                                          test_acc))\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_acc'].append(test_acc)\n",
    "\n",
    "#foldperf['fold{}'.format(fold+1)] = history  \n",
    "\n",
    "torch.save(model.state_dict(),PATH)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
